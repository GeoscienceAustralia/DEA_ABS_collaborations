{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabulation of Fractional Cover data within shapefile polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T04:13:20.577632Z",
     "start_time": "2019-02-04T04:13:20.554372Z"
    }
   },
   "source": [
    "**What does this notebook do?**\n",
    "\n",
    "This notebook is a pilot collaboration between Geoscience Australia and Australian Bureau of Statistics. The purpose of the notebook is to use a shapefile polygon boundaries to load fractional cover dataset, complete zonal statistics and tabulate the results.\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "You need to run the following commands from the command line prior to launching jupyter notebooks from the same terminal so that the required libraries and paths are set:\n",
    "\n",
    "`module use /g/data/v10/public/modules/modulefiles`\n",
    "\n",
    "`module load dea/20181213`\n",
    "\n",
    "\n",
    "**Background**\n",
    "\n",
    "Data from the Landsat 5,7 and 8 satellite missions are accessible through Digital Earth Australia (DEA). The code snippets in this notebook will let you retrieve and plot the Fractional Cover (FC25) data stored in DEA.\n",
    "\n",
    "\n",
    "**How to use this notebook**\n",
    "\n",
    "A basic understanding of any programming language is desirable but one doesn't have to be an expert Python programmer to manipulate the code to get and display the data.This doc applies to the following Landsat satellites, Fractional Cover bands and the WOfS dataset:\n",
    "\n",
    "- Landsat 5\n",
    "- Landsat 7\n",
    "- Landsat 8\n",
    "- PV - Photosythetic vegetation\n",
    "- NPV - Non-Photosythetic vegetation\n",
    "- BS - Bare Soil\n",
    "- UE - Unmixing Error\n",
    "- Water Observations from Space (WOFs)\n",
    "- WOfS Feature Layer (WOFL)\n",
    "\n",
    "**Bugs still to fix**\n",
    "\n",
    "- Memory errors for extra large polygons - AH & ET\n",
    "\n",
    "\n",
    "**Errors or bugs**\n",
    "\n",
    "If you find an error or bug in this notebook, please contact erin.telfer@ga.gov.au.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:17.530134Z",
     "start_time": "2019-03-25T01:52:08.144451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell finished at 2019-03-25 12:52:17\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import time, datetime\n",
    "import os.path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import csv\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "import fiona\n",
    "from datetime import datetime\n",
    "import dask\n",
    "from dask.delayed import delayed\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import tempfile\n",
    "\n",
    "import datacube\n",
    "from datacube import Datacube\n",
    "from datacube.virtual import construct, construct_from_yaml\n",
    "from datacube.ui.task_app import year_splitter\n",
    "from datacube.utils.geometry import CRS\n",
    "\n",
    "print(f\"Cell finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define directories and years of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:17.538685Z",
     "start_time": "2019-03-25T01:52:17.532879Z"
    }
   },
   "outputs": [],
   "source": [
    "#Set folder  and SA2 shapefile locations\n",
    "shapefile_path = '../input/SA2_2016_AUST.shp'\n",
    "output_path = '/g/data/r78/ext547/abs/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:22.881367Z",
     "start_time": "2019-03-25T01:52:22.875271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2011', '2012')\n"
     ]
    }
   ],
   "source": [
    "#Specify time years of interest\n",
    "start_year, end_year = 2011, 2012\n",
    "time_range = (str(start_year), str(end_year))\n",
    "print(time_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up a local dask cluster\n",
    "Some calculations take more memory than is available on a system.  By breaking the data up into chunks, we can chain a sequence of operations together, and work on the data a small piece at a time.\n",
    "\n",
    "This lets several processes work at the same time, and manage total memory usage for the calculations.\n",
    "\n",
    "In more advanced setups, we can distribute the work across multiple computers, using all of their memory and CPU power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:22.889864Z",
     "start_time": "2019-03-25T01:52:22.885450Z"
    }
   },
   "outputs": [],
   "source": [
    "#Set up dask cluster\n",
    "n_workers = 7\n",
    "threads_per_worker=1\n",
    "mem_per_worker = 8e9  # 8e9 is 8GB (8,000,000,000 bytes)\n",
    "\n",
    "chunk_size = {'time': 1, 'x': 2000, 'y': 2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:25.543443Z",
     "start_time": "2019-03-25T01:52:22.892674Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/dea-env/20181015/lib/python3.6/site-packages/distributed/bokeh/core.py:56: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn('\\n' + msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:33548\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:34310/status' target='_blank'>http://127.0.0.1:34310/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>7</li>\n",
       "  <li><b>Cores: </b>7</li>\n",
       "  <li><b>Memory: </b>56.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:33548' processes=7 cores=7>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = LocalCluster(local_dir=tempfile.gettempdir(), \n",
    "                       n_workers=n_workers, \n",
    "                       threads_per_worker=threads_per_worker,\n",
    "                       memory_limit=mem_per_worker)\n",
    "client = Client(cluster)\n",
    "dask.config.set(get=client.get)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also get a dashboard to see how the system is running, by clicking the link above after the cell has been run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Connect to the Datacube "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:26.822206Z",
     "start_time": "2019-03-25T01:52:25.546847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell finished at 2019-03-25 12:52:26\n"
     ]
    }
   ],
   "source": [
    "dc = Datacube()\n",
    "print(f\"Cell finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Construct the virtual product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:26.831833Z",
     "start_time": "2019-03-25T01:52:26.824982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell finished at 2019-03-25 12:52:26\n"
     ]
    }
   ],
   "source": [
    "#Remove Landsat 7 scenes with the Scan Line Correction (SLC) missing data\n",
    "LS7_BROKEN_DATE = datetime(2003, 5, 31)\n",
    "is_pre_slc_failure = lambda dataset: dataset.center_time < LS7_BROKEN_DATE\n",
    "print(f\"Cell finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:26.845042Z",
     "start_time": "2019-03-25T01:52:26.836899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell finished at 2019-03-25 12:52:26\n"
     ]
    }
   ],
   "source": [
    "#Create function to ensure wofls in correct format\n",
    "def wofls_fuser(dest, src):\n",
    "    where_nodata = (src & 1) == 0\n",
    "    numpy.copyto(dest, src, where=where_nodata)\n",
    "    return dest\n",
    "print(f\"Cell finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:26.868097Z",
     "start_time": "2019-03-25T01:52:26.849387Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create virtual product so that datacube data can be loaded effectively within memory\n",
    "fc_and_water_yaml = \"\"\"\n",
    "        juxtapose:\n",
    "          - collate:\n",
    "              - transform: apply_mask\n",
    "                mask_measurement_name: pixelquality\n",
    "                preserve_dtype: false\n",
    "                input:\n",
    "                    juxtapose:\n",
    "                      - product: ls5_fc_albers\n",
    "                        group_by: solar_day\n",
    "                        measurements: [PV, NPV, BS]\n",
    "                      - transform: make_mask\n",
    "                        input:\n",
    "                            product: ls5_pq_albers\n",
    "                            group_by: solar_day\n",
    "                            fuse_func: datacube.helpers.ga_pq_fuser\n",
    "                        flags:\n",
    "                            ga_good_pixel: true\n",
    "                        mask_measurement_name: pixelquality\n",
    "              - transform: apply_mask\n",
    "                mask_measurement_name: pixelquality\n",
    "                preserve_dtype: false\n",
    "                input:\n",
    "                    juxtapose:\n",
    "                      - product: ls7_fc_albers\n",
    "                        group_by: solar_day\n",
    "                        measurements: [PV, NPV, BS]\n",
    "                        # dataset_predicate: __main__.is_pre_slc_failure\n",
    "                      - transform: make_mask\n",
    "                        input:\n",
    "                            product: ls7_pq_albers\n",
    "                            group_by: solar_day\n",
    "                            fuse_func: datacube.helpers.ga_pq_fuser\n",
    "                        flags:\n",
    "                            ga_good_pixel: true\n",
    "                        mask_measurement_name: pixelquality\n",
    "              - transform: apply_mask\n",
    "                mask_measurement_name: pixelquality\n",
    "                preserve_dtype: false\n",
    "                input:\n",
    "                    juxtapose:\n",
    "                      - product: ls8_fc_albers\n",
    "                        group_by: solar_day\n",
    "                        measurements: [PV, NPV, BS]\n",
    "                      - transform: make_mask\n",
    "                        input:\n",
    "                            product: ls8_pq_albers\n",
    "                            group_by: solar_day\n",
    "                            fuse_func: datacube.helpers.ga_pq_fuser\n",
    "                        flags:\n",
    "                            ga_good_pixel: true\n",
    "                        mask_measurement_name: pixelquality\n",
    "          - transform: make_mask\n",
    "            input:\n",
    "                product: wofs_albers\n",
    "                group_by: solar_day\n",
    "                fuse_func: __main__.wofls_fuser\n",
    "            flags:\n",
    "                wet: true\n",
    "            mask_measurement_name: water\n",
    "\"\"\"\n",
    "fc_and_water = construct_from_yaml(fc_and_water_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:26.893165Z",
     "start_time": "2019-03-25T01:52:26.878031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell finished at 2019-03-25 12:52:26\n"
     ]
    }
   ],
   "source": [
    "def geometry_mask(geoms, geobox, all_touched=False, invert=False, chunks=None):\n",
    "    \"\"\"\n",
    "    Create a mask from shapes.\n",
    "\n",
    "    By default, mask is intended for use as a\n",
    "    numpy mask, where pixels that overlap shapes are False.\n",
    "    :param list[Geometry] geoms: geometries to be rasterized\n",
    "    :param datacube.utils.GeoBox geobox:\n",
    "    :param bool all_touched: If True, all pixels touched by geometries will be burned in. If\n",
    "                             false, only pixels whose center is within the polygon or that\n",
    "                             are selected by Bresenham's line algorithm will be burned in.\n",
    "    :param bool invert: If True, mask will be True for pixels that overlap shapes.\n",
    "    \"\"\"\n",
    "    data = rasterio.features.geometry_mask([geom.to_crs(geobox.crs) for geom in geoms],\n",
    "                                           out_shape=geobox.shape,\n",
    "                                           transform=geobox.affine,\n",
    "                                           all_touched=all_touched,\n",
    "                                           invert=invert)\n",
    "    if chunks is not None:\n",
    "        data = dask.array.from_array(data, chunks=tuple(chunks[d] for d in geobox.dims))\n",
    "        \n",
    "    coords = [xr.DataArray(data=coord.values, name=dim, dims=[dim], attrs={'units': coord.units}) \n",
    "              for dim, coord in geobox.coords.items()]\n",
    "    return xr.DataArray(data, coords=coords)\n",
    "print(f\"Cell finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:26.906183Z",
     "start_time": "2019-03-25T01:52:26.896061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell finished at 2019-03-25 12:52:26\n"
     ]
    }
   ],
   "source": [
    "def get_shapes(shape_file):\n",
    "    \"\"\"\n",
    "    Extract spatial inforamtion from polygons within shapefile\n",
    "    \"\"\"\n",
    "    with fiona.open(shape_file) as shapes:\n",
    "        crs = datacube.utils.geometry.CRS(shapes.crs_wkt)\n",
    "        for shape in shapes:\n",
    "            if shape['geometry'] is None:\n",
    "                continue\n",
    "            geom = datacube.utils.geometry.Geometry(shape['geometry'], crs=crs)\n",
    "            geom = geom.to_crs(CRS('EPSG:3577'))\n",
    "            yield geom, shape['properties']\n",
    "print(f\"Cell finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:26.926424Z",
     "start_time": "2019-03-25T01:52:26.909015Z"
    }
   },
   "outputs": [],
   "source": [
    "def fc_and_water_summary(data, mask_int):\n",
    "    \"\"\"\n",
    "    Calculate the percentage and the area of each FC component, \n",
    "    water, and unclassified data within each SA2 boundary. \n",
    "    \"\"\"    \n",
    "    # Where there is water, set the FC bands to 0%\n",
    "    fc = data[['PV', 'NPV', 'BS']].where(data.water==0, other=0)\n",
    "    fc['water'] = data.water.where(numpy.isfinite(fc['BS'])) * numpy.float32(100)\n",
    "    fc = fc.apply(lambda data_array: data_array.clip(0, 100).where(numpy.isfinite(data_array)))\n",
    "    \n",
    "    # Flatten to a monthly mean\n",
    "    fc = fc.groupby(fc.time.astype('datetime64[M]'), squeeze=False).mean(dim='time', skipna=True)\n",
    "    # Calculate percentage of cover based on area of mask\n",
    "    percentage = fc.sum(dim=['x','y']) * (100 / mask_int)\n",
    "    \n",
    "    for da in percentage.data_vars.values():\n",
    "        da.attrs['units'] = '%'\n",
    "\n",
    "    fc_unclass = percentage.to_array('variable').sum(dim='variable')\n",
    "    fc_unclass = 100 - float(fc_unclass.values)\n",
    "    print(f\"   PV = {int(percentage.PV.values)}%, NPV = {int(percentage.NPV.values)}%, BS = {int(percentage.BS.values)}%, water = {int(percentage.water.values)}%, unclassified = {int(fc_unclass)}%\")\n",
    "    \n",
    "    fc['unclassified'] = ('time', numpy.repeat(fc_unclass,fc.time.size)) \n",
    "    percentage['unclassified'] = ('time', numpy.repeat(fc_unclass,fc.time.size)) \n",
    "        \n",
    "    pixel_area_in_metres2 = 25 * 25\n",
    "    m2_to_km2 = (1 / 1_000_000)\n",
    "    percent_to_fraction = (1 / 100)\n",
    "    \n",
    "    area = (fc * (pixel_area_in_metres2 * m2_to_km2 * percent_to_fraction)).sum(dim=['x','y'])\n",
    "    area = area.rename({'BS': 'BS_area', \n",
    "                        'PV': 'PV_area', \n",
    "                        'NPV': 'NPV_area', \n",
    "                        'water': 'water_area', \n",
    "                        'unclassified':'unclassified_area'})\n",
    "    \n",
    "    for da in area.data_vars.values():\n",
    "        da.attrs['units'] = 'km^2'\n",
    "    fc = percentage.merge(area)\n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:26.945951Z",
     "start_time": "2019-03-25T01:52:26.930017Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell finished at 2019-03-25 12:52:26\n"
     ]
    }
   ],
   "source": [
    "def plot_stacked(ds, sa2_id,plot_title='title', show=True):\n",
    "    \"\"\"\n",
    "    Create and save a stacked plot to visualise FC components\n",
    "    \"\"\"\n",
    "    if not show:\n",
    "        plt.ioff()\n",
    "        \n",
    "    fig,ax = plt.subplots(figsize=(10,5))\n",
    "    ax.stackplot(ds.dropna(dim='time').time.data, \n",
    "                 ds.dropna(dim='time').PV,\n",
    "                 ds.dropna(dim='time').NPV, \n",
    "                 ds.dropna(dim='time').BS, \n",
    "                 ds.dropna(dim='time').water, \n",
    "                 ds.dropna(dim='time').unclassified, \n",
    "                 colors = ['darkolivegreen','olive','tan','darkblue','red'], \n",
    "                 labels=['PV','NPV','BS','Water','Unclassified',])\n",
    "    plt.legend(loc='upper center', ncol = 5)\n",
    "    plt.title(f'FC components: SA2 ID {sa2_id}', size=12)\n",
    "    plt.ylabel('Percentage (%)', size=12) #Set Y label\n",
    "    plt.xlabel('Date', size=12) #Set X label\n",
    "    \n",
    "    plt.savefig(f'{output_path}/{sa2_id}_{plot_title}.png');\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Turn interactive back on\n",
    "    if not show:\n",
    "        plt.show()\n",
    "print(f\"Cell finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:26.954725Z",
     "start_time": "2019-03-25T01:52:26.949094Z"
    }
   },
   "outputs": [],
   "source": [
    "def month_splitter(start_year, end_year_inclusive):\n",
    "    \"\"\" \n",
    "    Split specified years into months \n",
    "    \"\"\"\n",
    "    yield from (str(p) for p in pd.period_range(start=pd.Period(start_year).start_time, \n",
    "                               end=pd.Period(end_year_inclusive).end_time, \n",
    "                               freq='M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:26.964816Z",
     "start_time": "2019-03-25T01:52:26.957135Z"
    }
   },
   "outputs": [],
   "source": [
    "def output_csv(input_ds, sa2_id, sa2_name, sa2_size, monthly_or_annual='frequency'):\n",
    "    \"\"\"\n",
    "    Save tabulated data into a csv\n",
    "    \"\"\"\n",
    "    input_ds = input_ds.to_dataframe()\n",
    "    input_ds.insert(0,'SA2_id',sa2_id)\n",
    "    input_ds.insert(1,'SA2_name',sa2_name)\n",
    "    input_ds.insert(2,'SA2_size',sa2_size)\n",
    "    input_ds.to_csv(f\"{output_path}/tabulate_{monthly_or_annual}_FC.csv\",mode='a',header=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set up the query\n",
    "For each year and polygon query the product, apply the geometry mask and compute the fractional cover stats\n",
    "\n",
    "Using `client.compute()` lets us use the monthly results in calculating the annual results at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:40.520925Z",
     "start_time": "2019-03-25T01:52:28.200297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell finished at 2019-03-25 12:52:40\n"
     ]
    }
   ],
   "source": [
    "#Obtain spatial information from shapefile\n",
    "shape_file = os.path.expanduser(f'{shapefile_path}')\n",
    "shapes = list(get_shapes(shape_file))\n",
    "\n",
    "# #Specify a particular SA2 boundary, if required\n",
    "# shapes = [(g,p) for g, p in shapes if str(p['SA2_MAIN16']) == '312011340']\n",
    "shapes = [(g,p) for g, p in shapes if str(p['AREASQKM16']) < '15_000']\n",
    "\n",
    "\n",
    "print(f\"Cell finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have enough resources, we can start the query and calculation of the next year's data while the previous is still being calculated. `by_slice=False` will be faster, but use more memory.\n",
    "\n",
    "For larger areas `by_slice` will need to be `True`, so that the compute cluster does not become overwhelmed.  \n",
    "\n",
    "If you get the error:\n",
    "> `distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting`\n",
    "\n",
    "then you will need to set `by_slice=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:40.530601Z",
     "start_time": "2019-03-25T01:52:40.525050Z"
    }
   },
   "outputs": [],
   "source": [
    "by_slice=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:40.554933Z",
     "start_time": "2019-03-25T01:52:40.534177Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_area(geometry, sa2_id, sa2_name, sa2_size, time_range):\n",
    "    monthly_values = []\n",
    "    annual_values = []\n",
    "    mask = None\n",
    "\n",
    "    for sub_time_range in month_splitter(time_range[0], time_range[-1]):\n",
    "        print(sub_time_range)\n",
    "        try:\n",
    "            data = fc_and_water.load(dc, dask_chunks=chunk_size, \n",
    "                                     time=sub_time_range, \n",
    "                                     geopolygon=geometry)\n",
    "        except ValueError:\n",
    "            print(f'    No data for {sub_time_range} , skipping...')\n",
    "            continue\n",
    "\n",
    "        if mask is None:\n",
    "            mask = geometry_mask([geometry], data.geobox, invert=True, chunks=data.chunks)\n",
    "            mask_int = mask * 1\n",
    "            mask_int = mask_int.sum() * 100\n",
    "        data = data.where(mask)\n",
    "        monthly_data = fc_and_water_summary(data, mask_int)\n",
    "        monthly_data = client.compute(monthly_data, sync=by_slice)\n",
    "        monthly_values.append(monthly_data)\n",
    "        \n",
    "    if not by_slice:\n",
    "        print(\"  all years queried, hard load data\")\n",
    "        monthly_values = client.gather(monthly_values)\n",
    "\n",
    "    monthly_values = xr.concat(monthly_values, dim='time')\n",
    "    monthly_values = monthly_values.where(monthly_values['unclassified'] < 10).dropna(dim='time')\n",
    "    annual_values = monthly_values.resample(time='1YS').mean(dim='time', skipna=True)\n",
    "    print(monthly_values)\n",
    "    print(annual_values)\n",
    "    print(f\"Calculation complete for annual values\")\n",
    "    \n",
    "    plot_stacked(monthly_values, sa2_id, plot_title='monthly_plot_wofs',show=False)\n",
    "    plot_stacked(annual_values, sa2_id, plot_title='annual_plot_wofs', show=False)\n",
    "    \n",
    "    print(\"All data loaded, save to csv\")\n",
    "    output_csv(monthly_values, sa2_id, sa2_name, sa2_size, monthly_or_annual='monthly')\n",
    "    output_csv(annual_values, sa2_id, sa2_name, sa2_size, monthly_or_annual='annual')\n",
    "    \n",
    "    print(f\"SA2 {sa2_id} done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Process query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:52:40.646126Z",
     "start_time": "2019-03-25T01:52:40.560777Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create empty CSV with specified headings\n",
    "header = ['DATE','SA2_ID', 'SA2_NAME, SA2_SIZE','PV_PERCENTAGE','NPV_PERCENTAGE','BS_PERCENTAGE','WOFL_PERCENTAGE',\n",
    "          'UNCLASSIFIED_PERCENTAGE','PV_AREA_SQKM_ALBERS3577','NPV_AREA_SQKM_ALBERS3577',\n",
    "          'BS_AREA_SQKM_ALBERS3577','WOFL_AREA_SQKM_ALBERS3577','UNCLASSIFIED_AREA_SQKM_ALBERS3577']\n",
    "\n",
    "with open(f\"{output_path}/tabulate_annual_FC.csv\",\"w\") as outcsv: #create csv to save output and add header text\n",
    "    writer = csv.writer(outcsv)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "with open(f\"{output_path}/tabulate_monthly_FC.csv\",\"w\") as outcsv: #create csv to save output and add header text\n",
    "    writer = csv.writer(outcsv)\n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T01:57:56.567705Z",
     "start_time": "2019-03-25T01:52:40.648881Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA2 ID: 101021010, size: 101021010km^2, time: ('2011', '2012')\n",
      "2011-01\n",
      "   PV = 46%, NPV = 32%, BS = 19%, water = 0%, unclassified = 1%\n",
      "2011-02\n",
      "   PV = 0%, NPV = 0%, BS = 0%, water = 0%, unclassified = 100%\n",
      "2011-03\n",
      "   PV = 48%, NPV = 30%, BS = 19%, water = 0%, unclassified = 1%\n",
      "2011-04\n",
      "   PV = 42%, NPV = 27%, BS = 20%, water = 0%, unclassified = 7%\n",
      "2011-05\n",
      "   PV = 34%, NPV = 26%, BS = 20%, water = 0%, unclassified = 18%\n",
      "2011-06\n",
      "   PV = 33%, NPV = 25%, BS = 17%, water = 0%, unclassified = 23%\n",
      "2011-07\n",
      "   PV = 39%, NPV = 34%, BS = 22%, water = 0%, unclassified = 3%\n",
      "2011-08\n",
      "   PV = 27%, NPV = 25%, BS = 27%, water = 0%, unclassified = 19%\n",
      "2011-09\n",
      "   PV = 36%, NPV = 38%, BS = 23%, water = 0%, unclassified = 1%\n",
      "2011-10\n",
      "   PV = 33%, NPV = 32%, BS = 17%, water = 0%, unclassified = 16%\n",
      "2011-11\n",
      "   PV = 6%, NPV = 4%, BS = 7%, water = 0%, unclassified = 81%\n",
      "2011-12\n",
      "   PV = 26%, NPV = 23%, BS = 15%, water = 0%, unclassified = 34%\n",
      "2012-01\n",
      "   PV = 37%, NPV = 34%, BS = 25%, water = 0%, unclassified = 1%\n",
      "2012-02\n",
      "   PV = 29%, NPV = 26%, BS = 15%, water = 0%, unclassified = 28%\n",
      "2012-03\n",
      "   PV = 31%, NPV = 25%, BS = 16%, water = 0%, unclassified = 27%\n",
      "2012-04\n",
      "   PV = 10%, NPV = 6%, BS = 11%, water = 0%, unclassified = 70%\n",
      "2012-05\n",
      "   PV = 39%, NPV = 32%, BS = 21%, water = 0%, unclassified = 6%\n",
      "2012-06\n",
      "   PV = 29%, NPV = 24%, BS = 16%, water = 0%, unclassified = 30%\n",
      "2012-07\n",
      "   PV = 31%, NPV = 29%, BS = 20%, water = 0%, unclassified = 18%\n",
      "2012-08\n",
      "   PV = 34%, NPV = 33%, BS = 27%, water = 0%, unclassified = 5%\n",
      "2012-09\n",
      "   PV = 36%, NPV = 37%, BS = 21%, water = 0%, unclassified = 4%\n",
      "2012-10\n",
      "   PV = 28%, NPV = 23%, BS = 13%, water = 0%, unclassified = 34%\n",
      "2012-11\n",
      "   PV = 37%, NPV = 31%, BS = 18%, water = 0%, unclassified = 11%\n",
      "2012-12\n",
      "   PV = 34%, NPV = 30%, BS = 15%, water = 0%, unclassified = 18%\n",
      "<xarray.Dataset>\n",
      "Dimensions:            (time: 9)\n",
      "Coordinates:\n",
      "  * time               (time) datetime64[ns] 2011-01-01 ... 2012-09-01\n",
      "Data variables:\n",
      "    PV                 (time) float64 46.83 48.0 42.52 ... 39.25 34.22 36.69\n",
      "    NPV                (time) float64 32.64 30.98 27.67 ... 32.55 33.04 37.16\n",
      "    BS                 (time) float64 19.22 19.69 21.0 ... 21.52 27.43 21.1\n",
      "    water              (time) float64 0.02404 0.09375 0.8846 ... 0.05769 0.04808\n",
      "    unclassified       (time) float64 1.283 1.23 7.934 ... 6.664 5.249 4.999\n",
      "    PV_area            (time) float32 6.0884905 6.2401958 ... 4.449003 4.7699842\n",
      "    NPV_area           (time) float32 4.2426434 4.0279937 ... 4.295537 4.8305345\n",
      "    BS_area            (time) float32 2.4989376 2.5597582 ... 3.565628 2.7433314\n",
      "    water_area         (time) float64 0.003125 0.01219 0.115 ... 0.0075 0.00625\n",
      "    unclassified_area  (time) float64 8.019e-06 7.686e-06 ... 3.28e-05 3.125e-05\n",
      "<xarray.Dataset>\n",
      "Dimensions:            (time: 2)\n",
      "Coordinates:\n",
      "  * time               (time) datetime64[ns] 2011-01-01 2012-01-01\n",
      "Data variables:\n",
      "    PV                 (time) float64 42.76 37.03\n",
      "    NPV                (time) float64 32.75 34.4\n",
      "    BS                 (time) float64 21.22 23.82\n",
      "    water              (time) float64 0.2101 0.03185\n",
      "    unclassified       (time) float64 3.058 4.721\n",
      "    PV_area            (time) float32 5.558638 4.813699\n",
      "    NPV_area           (time) float32 4.258003 4.4717937\n",
      "    BS_area            (time) float32 2.7585075 3.0966501\n",
      "    water_area         (time) float64 0.02731 0.004141\n",
      "    unclassified_area  (time) float64 1.911e-05 2.951e-05\n",
      "Calculation complete for annual values\n",
      "All data loaded, save to csv\n",
      "SA2 101021010 done\n",
      "SA2 ID: 101021012, size: 101021012km^2, time: ('2011', '2012')\n",
      "2011-01\n",
      "   PV = 44%, NPV = 30%, BS = 23%, water = 0%, unclassified = 1%\n",
      "2011-02\n",
      "    No data for 2011-02 , skipping...\n",
      "2011-03\n",
      "   PV = 45%, NPV = 29%, BS = 21%, water = 0%, unclassified = 3%\n",
      "2011-04\n",
      "   PV = 36%, NPV = 24%, BS = 25%, water = 0%, unclassified = 13%\n",
      "2011-05\n",
      "   PV = 14%, NPV = 13%, BS = 19%, water = 0%, unclassified = 52%\n",
      "2011-06\n",
      "   PV = 24%, NPV = 30%, BS = 24%, water = 0%, unclassified = 20%\n",
      "2011-07\n",
      "   PV = 29%, NPV = 36%, BS = 32%, water = 0%, unclassified = 1%\n",
      "2011-08\n",
      "   PV = 1%, NPV = 3%, BS = 0%, water = 0%, unclassified = 95%\n",
      "2011-09\n",
      "   PV = 31%, NPV = 38%, BS = 28%, water = 0%, unclassified = 1%\n",
      "2011-10\n",
      "   PV = 34%, NPV = 28%, BS = 22%, water = 0%, unclassified = 14%\n",
      "2011-11\n",
      "   PV = 38%, NPV = 33%, BS = 25%, water = 0%, unclassified = 2%\n",
      "2011-12\n",
      "   PV = 32%, NPV = 26%, BS = 19%, water = 0%, unclassified = 21%\n",
      "2012-01\n",
      "   PV = 39%, NPV = 31%, BS = 26%, water = 0%, unclassified = 1%\n",
      "2012-02\n",
      "   PV = 32%, NPV = 25%, BS = 19%, water = 0%, unclassified = 22%\n",
      "2012-03\n",
      "   PV = 38%, NPV = 20%, BS = 21%, water = 0%, unclassified = 18%\n",
      "2012-04\n",
      "   PV = 6%, NPV = 4%, BS = 5%, water = 0%, unclassified = 83%\n",
      "2012-05\n",
      "   PV = 35%, NPV = 29%, BS = 27%, water = 0%, unclassified = 8%\n",
      "2012-06\n",
      "   PV = 24%, NPV = 23%, BS = 19%, water = 0%, unclassified = 32%\n",
      "2012-07\n",
      "   PV = 35%, NPV = 27%, BS = 24%, water = 0%, unclassified = 11%\n",
      "2012-08\n",
      "   PV = 27%, NPV = 29%, BS = 29%, water = 0%, unclassified = 13%\n",
      "2012-09\n",
      "   PV = 35%, NPV = 32%, BS = 27%, water = 0%, unclassified = 4%\n",
      "2012-10\n",
      "   PV = 33%, NPV = 20%, BS = 26%, water = 0%, unclassified = 19%\n",
      "2012-11\n",
      "   PV = 43%, NPV = 27%, BS = 25%, water = 0%, unclassified = 3%\n",
      "2012-12\n",
      "   PV = 32%, NPV = 27%, BS = 20%, water = 0%, unclassified = 19%\n",
      "<xarray.Dataset>\n",
      "Dimensions:            (time: 9)\n",
      "Coordinates:\n",
      "  * time               (time) datetime64[ns] 2011-01-01 ... 2012-11-01\n",
      "Data variables:\n",
      "    PV                 (time) float64 44.14 45.36 29.13 ... 35.51 35.05 43.27\n",
      "    NPV                (time) float64 30.47 29.03 36.81 ... 29.08 32.11 27.29\n",
      "    BS                 (time) float64 23.91 21.64 32.46 ... 27.05 27.65 25.86\n",
      "    water              (time) float64 0.2239 0.2604 0.0 ... 0.3563 0.09822\n",
      "    unclassified       (time) float64 1.259 3.71 1.597 ... 8.351 4.826 3.48\n",
      "    PV_area            (time) float32 6.0380187 6.2051873 ... 5.9200683\n",
      "    NPV_area           (time) float32 4.168556 3.9718626 ... 4.393125 3.732831\n",
      "    BS_area            (time) float32 3.2711313 2.9603875 ... 3.5382624\n",
      "    water_area         (time) float64 0.03062 0.03563 0.0 ... 0.04875 0.01344\n",
      "    unclassified_area  (time) float64 7.871e-06 2.319e-05 ... 2.175e-05\n",
      "<xarray.Dataset>\n",
      "Dimensions:            (time: 2)\n",
      "Coordinates:\n",
      "  * time               (time) datetime64[ns] 2011-01-01 2012-01-01\n",
      "Data variables:\n",
      "    PV                 (time) float64 37.79 38.34\n",
      "    NPV                (time) float64 33.69 30.01\n",
      "    BS                 (time) float64 26.29 26.88\n",
      "    water              (time) float64 0.2138 0.1867\n",
      "    unclassified       (time) float64 2.019 4.583\n",
      "    PV_area            (time) float32 5.1704235 5.244772\n",
      "    NPV_area           (time) float32 4.608538 4.105712\n",
      "    BS_area            (time) float32 3.596187 3.677606\n",
      "    water_area         (time) float64 0.02925 0.02555\n",
      "    unclassified_area  (time) float64 1.262e-05 2.864e-05\n",
      "Calculation complete for annual values\n",
      "All data loaded, save to csv\n",
      "SA2 101021012 done\n",
      "SA2 ID: 101031014, size: 101031014km^2, time: ('2011', '2012')\n",
      "2011-01\n",
      "   PV = 45%, NPV = 44%, BS = 8%, water = 0%, unclassified = 1%\n",
      "2011-02\n",
      "   PV = 0%, NPV = 0%, BS = 0%, water = 0%, unclassified = 100%\n",
      "2011-03\n",
      "   PV = 56%, NPV = 27%, BS = 14%, water = 0%, unclassified = 1%\n",
      "2011-04\n",
      "   PV = 51%, NPV = 28%, BS = 9%, water = 0%, unclassified = 9%\n",
      "2011-05\n",
      "   PV = 39%, NPV = 36%, BS = 8%, water = 0%, unclassified = 15%\n",
      "2011-06\n",
      "   PV = 26%, NPV = 41%, BS = 10%, water = 0%, unclassified = 20%\n",
      "2011-07\n",
      "   PV = 30%, NPV = 53%, BS = 14%, water = 0%, unclassified = 1%\n",
      "2011-08\n",
      "   PV = 28%, NPV = 45%, BS = 12%, water = 0%, unclassified = 13%\n",
      "2011-09\n",
      "   PV = 34%, NPV = 53%, BS = 11%, water = 0%, unclassified = 1%\n",
      "2011-10\n",
      "   PV = 44%, NPV = 37%, BS = 10%, water = 0%, unclassified = 8%\n",
      "2011-11\n",
      "   PV = 0%, NPV = 0%, BS = 0%, water = 0%, unclassified = 100%\n",
      "2011-12\n",
      "   PV = 37%, NPV = 30%, BS = 8%, water = 0%, unclassified = 23%\n",
      "2012-01\n",
      "   PV = 41%, NPV = 42%, BS = 10%, water = 0%, unclassified = 5%\n",
      "2012-02\n",
      "   PV = 41%, NPV = 39%, BS = 6%, water = 0%, unclassified = 13%\n",
      "2012-03\n",
      "   PV = 8%, NPV = 5%, BS = 4%, water = 0%, unclassified = 81%\n",
      "2012-04\n",
      "   PV = 8%, NPV = 4%, BS = 3%, water = 0%, unclassified = 82%\n",
      "2012-05\n",
      "   PV = 45%, NPV = 43%, BS = 9%, water = 0%, unclassified = 1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-06\n",
      "   PV = 5%, NPV = 10%, BS = 6%, water = 0%, unclassified = 77%\n",
      "2012-07\n",
      "   PV = 7%, NPV = 12%, BS = 3%, water = 0%, unclassified = 76%\n",
      "2012-08\n",
      "   PV = 20%, NPV = 29%, BS = 10%, water = 0%, unclassified = 39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Worker process 21908 was killed by unknown signal\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n",
      "distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-01d2955cd6fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           \u001b[0mprocess_area\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeometry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa2_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa2_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa2_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not process {sa2_id}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-edfdb67f85d5>\u001b[0m in \u001b[0;36mprocess_area\u001b[0;34m(geometry, sa2_id, sa2_name, sa2_size, time_range)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmonthly_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfc_and_water_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mmonthly_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonthly_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mby_slice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mmonthly_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonthly_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20181015/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, collections, sync, optimize_graph, workers, allow_other_workers, resources, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2404\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2405\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2406\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20181015/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, maxsize, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             return self.sync(self._gather, futures, errors=errors,\n\u001b[1;32m   1592\u001b[0m                              \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m                              asynchronous=asynchronous)\n\u001b[0m\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoroutine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20181015/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20181015/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20181015/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/g/data/v10/public/modules/dea-env/20181015/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Run tabulation script\n",
    "for geometry, properties in shapes:\n",
    "    sa2_id = str(properties['SA2_MAIN16'])\n",
    "    sa2_name = str(properties['SA2_MAIN16'])\n",
    "    sa2_size = str(properties['SA2_MAIN16'])\n",
    "    print(f\"SA2 ID: {sa2_id}, size: {sa2_size}km^2, time: {time_range}\")\n",
    "    \n",
    "    try:\n",
    "          process_area(geometry, sa2_id, sa2_name, sa2_size, time_range)\n",
    "    except Exception as e:\n",
    "          print(f\"Could not process {sa2_id}: {e}\")\n",
    "          client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
