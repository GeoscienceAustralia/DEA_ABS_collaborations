{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabulation of Fractional Cover data within shapefile polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T04:13:20.577632Z",
     "start_time": "2019-02-04T04:13:20.554372Z"
    }
   },
   "source": [
    "**What does this notebook do?**\n",
    "\n",
    "This notebook is a pilot collaboration between Geoscience Australia and Australian Bureau of Statistics. The purpose of the notebook is to use a shapefile of SA2 boundaries to load Fractional Cover (FC) dataset, complete zonal statistics for each SA2 boundary, and  then tabulate monthly and annual results. Tabulated results show the percentage of the SA2 area that is classified as green vegetation (Photosythetic vegetation (PV)), brown vegetation (Non-Photosythetic vegetation (NPV)), bare soil (BS), water (open surface water), or unclassified (cloud or other). \n",
    "\n",
    "**Requirements**\n",
    "\n",
    "You need to run the following commands from the command line prior to launching jupyter notebooks from the same terminal so that the required libraries and paths are set:\n",
    "\n",
    "`module use /g/data/v10/public/modules/modulefiles`\n",
    "\n",
    "`module load dea/20181213`\n",
    "\n",
    "\n",
    "**Background**\n",
    "\n",
    "Data from the Landsat 5,7 and 8 satellite missions are accessible through Digital Earth Australia (DEA). The code snippets in this notebook will let you retrieve tabulate the FC data stored in DEA.\n",
    "\n",
    "\n",
    "**How to use this notebook**\n",
    "\n",
    "A basic understanding of any programming language is desirable but one doesn't have to be an expert Python programmer to manipulate the code to get and display the data.This doc applies to the following Landsat satellites, Fractional Cover bands and the WOfS dataset:\n",
    "\n",
    "- Landsat 5\n",
    "- Landsat 7\n",
    "- Landsat 8\n",
    "- PV - Photosythetic vegetation\n",
    "- NPV - Non-Photosythetic vegetation\n",
    "- BS - Bare Soil\n",
    "- UE - Unmixing Error\n",
    "- Water Observations from Space (WOFs)\n",
    "- WOfS Feature Layer (WOFL)\n",
    "\n",
    "**Bugs still to fix**\n",
    "\n",
    "- NA\n",
    "\n",
    "\n",
    "**Errors or bugs**\n",
    "\n",
    "If you find an error or bug in this notebook, please contact erin.telfer@ga.gov.au.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:09.761266Z",
     "start_time": "2019-09-15T22:50:34.556104Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from datetime import time, datetime\n",
    "import os.path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import csv\n",
    "import xarray as xr\n",
    "import rasterio.features\n",
    "import fiona\n",
    "import dask\n",
    "from dask.delayed import delayed\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import tempfile\n",
    "from datetime import *\n",
    "\n",
    "import datacube\n",
    "from datacube import Datacube\n",
    "from datacube.virtual import construct, construct_from_yaml\n",
    "from datacube.utils.geometry import CRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. USER INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:09.769466Z",
     "start_time": "2019-09-15T22:51:09.764414Z"
    }
   },
   "outputs": [],
   "source": [
    "#Do you want to create stackplots for each SA2? If yes, enter \"True\". If no, enter \"False\"\n",
    "create_stackplots = \"False\"\n",
    "\n",
    "#What years are you interested in?\n",
    "start_year = 2015\n",
    "end_year = 2017\n",
    "\n",
    "#Enter input batch details to help track each run of the code\n",
    "initials = 'ET' #initial of the person running the code\n",
    "batch_sa2_size = '0_to_10km2' #if you have split the SA2 into smaller subsets of data by size, add size range\n",
    "run_date = '16Aug19' #the date today\n",
    "run_notes = 'Rerun of SA2 between 0 to 10 km2 for the date range 2015 to 2017' #a brief note to describe the run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-29T04:50:38.748922Z",
     "start_time": "2019-05-29T04:50:38.744798Z"
    }
   },
   "source": [
    "## 3. Set directory and shapefile details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:09.779354Z",
     "start_time": "2019-09-15T22:51:09.773216Z"
    }
   },
   "outputs": [],
   "source": [
    "#Set folder location and shapefile name\n",
    "shapefile_path = '/g/data/r78/ext547/abs/input/SA2_2016_AUST.shp'\n",
    "#Set output folder location\n",
    "output_path = '/g/data/r78/ext547/abs/output/'\n",
    "#Save variable name for output csv\n",
    "name_of_output_file =f'tabulate_FC_{run_date}_{batch_sa2_size}_{start_year}_{end_year}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:09.790747Z",
     "start_time": "2019-09-15T22:51:09.782543Z"
    }
   },
   "outputs": [],
   "source": [
    "#Years of interest are saved in expected format\n",
    "time_range = (str(start_year), str(end_year))\n",
    "print(time_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set up a local dask cluster\n",
    "Some calculations take more memory than is available on a system.  By breaking the data up into chunks, we can chain a sequence of operations together, and work on the data a small piece at a time.\n",
    "\n",
    "This lets several processes work at the same time, and manage total memory usage for the calculations.\n",
    "\n",
    "In more advanced setups, we can distribute the work across multiple computers, using all of their memory and CPU power.\n",
    "\n",
    "* We set `n_workers` to be the number of worker applications we want to run in the background, doing the processing of the chunk-based steps we have chained together.\n",
    "* The `mem_per_worker` setting defines how much memory at most is available to each of the workers.\n",
    "* `chunk_size` sets the width and height of the chunk in pixels of the size will break up the data into.\n",
    "\n",
    "VDI has 8 CPUs available, and a total of 32GB of memory, but you will typically be sharing those with several (2-10) other users.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:09.798718Z",
     "start_time": "2019-09-15T22:51:09.793465Z"
    }
   },
   "outputs": [],
   "source": [
    "#Set up dask cluster\n",
    "n_workers = 7\n",
    "threads_per_worker=1\n",
    "mem_per_worker = 8e9  # 8e9 is 8GB (8,000,000,000 bytes)\n",
    "\n",
    "chunk_size = {'time': 1, 'x': 2000, 'y': 2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:17.505703Z",
     "start_time": "2019-09-15T22:51:09.801461Z"
    }
   },
   "outputs": [],
   "source": [
    "cluster = LocalCluster(local_dir=tempfile.gettempdir(), \n",
    "                       n_workers=n_workers, \n",
    "                       threads_per_worker=threads_per_worker,\n",
    "                       memory_limit=mem_per_worker)\n",
    "client = Client(cluster)\n",
    "dask.config.set(get=client.get)\n",
    "by_slice=True\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also get a dashboard to see how the system is running, by clicking the link above after the cell has been run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Connect to the Datacube "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:18.676856Z",
     "start_time": "2019-09-15T22:51:17.509739Z"
    }
   },
   "outputs": [],
   "source": [
    "dc = Datacube()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Construct the virtual product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:18.689346Z",
     "start_time": "2019-09-15T22:51:18.683636Z"
    }
   },
   "outputs": [],
   "source": [
    "#Remove Landsat 7 scenes with the Scan Line Correction (SLC) missing data\n",
    "LS7_BROKEN_DATE = datetime(2003, 5, 31)\n",
    "is_pre_slc_failure = lambda dataset: dataset.center_time < LS7_BROKEN_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:18.701163Z",
     "start_time": "2019-09-15T22:51:18.693653Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create function to ensure wofls in correct format\n",
    "def wofls_fuser(dest, src):\n",
    "    where_nodata = (src & 1) == 0\n",
    "    numpy.copyto(dest, src, where=where_nodata)\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:18.726745Z",
     "start_time": "2019-09-15T22:51:18.704022Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create virtual product so that datacube data can be loaded effectively within memory\n",
    "fc_and_water_yaml = \"\"\"\n",
    "        juxtapose:\n",
    "          - collate:\n",
    "              - transform: apply_mask\n",
    "                mask_measurement_name: pixelquality\n",
    "                preserve_dtype: false\n",
    "                input:\n",
    "                    juxtapose:\n",
    "                      - product: ls5_fc_albers\n",
    "                        group_by: solar_day\n",
    "                        measurements: [PV, NPV, BS]\n",
    "                      - transform: make_mask\n",
    "                        input:\n",
    "                            product: ls5_pq_albers\n",
    "                            group_by: solar_day\n",
    "                            fuse_func: datacube.helpers.ga_pq_fuser\n",
    "                        flags:\n",
    "                            ga_good_pixel: true\n",
    "                        mask_measurement_name: pixelquality\n",
    "              - transform: apply_mask\n",
    "                mask_measurement_name: pixelquality\n",
    "                preserve_dtype: false\n",
    "                input:\n",
    "                    juxtapose:\n",
    "                      - product: ls7_fc_albers\n",
    "                        group_by: solar_day\n",
    "                        measurements: [PV, NPV, BS]\n",
    "                        # dataset_predicate: __main__.is_pre_slc_failure\n",
    "                      - transform: make_mask\n",
    "                        input:\n",
    "                            product: ls7_pq_albers\n",
    "                            group_by: solar_day\n",
    "                            fuse_func: datacube.helpers.ga_pq_fuser\n",
    "                        flags:\n",
    "                            ga_good_pixel: true\n",
    "                        mask_measurement_name: pixelquality\n",
    "              - transform: apply_mask\n",
    "                mask_measurement_name: pixelquality\n",
    "                preserve_dtype: false\n",
    "                input:\n",
    "                    juxtapose:\n",
    "                      - product: ls8_fc_albers\n",
    "                        group_by: solar_day\n",
    "                        measurements: [PV, NPV, BS]\n",
    "                      - transform: make_mask\n",
    "                        input:\n",
    "                            product: ls8_pq_albers\n",
    "                            group_by: solar_day\n",
    "                            fuse_func: datacube.helpers.ga_pq_fuser\n",
    "                        flags:\n",
    "                            ga_good_pixel: true\n",
    "                        mask_measurement_name: pixelquality\n",
    "          - transform: make_mask\n",
    "            input:\n",
    "                product: wofs_albers\n",
    "                group_by: solar_day\n",
    "                fuse_func: __main__.wofls_fuser\n",
    "            flags:\n",
    "                wet: true\n",
    "            mask_measurement_name: water\n",
    "\"\"\"\n",
    "fc_and_water = construct_from_yaml(fc_and_water_yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:18.741117Z",
     "start_time": "2019-09-15T22:51:18.730012Z"
    }
   },
   "outputs": [],
   "source": [
    "def geometry_mask(geoms, geobox, all_touched=False, invert=False, chunks=None):\n",
    "    \"\"\"\n",
    "    Create a mask from shapes.\n",
    "\n",
    "    By default, mask is intended for use as a\n",
    "    numpy mask, where pixels that overlap shapes are False.\n",
    "    :param list[Geometry] geoms: geometries to be rasterized\n",
    "    :param datacube.utils.GeoBox geobox:\n",
    "    :param bool all_touched: If True, all pixels touched by geometries will be burned in. If\n",
    "                             false, only pixels whose center is within the polygon or that\n",
    "                             are selected by Bresenham's line algorithm will be burned in.\n",
    "    :param bool invert: If True, mask will be True for pixels that overlap shapes.\n",
    "    \"\"\"\n",
    "    data = rasterio.features.geometry_mask([geom.to_crs(geobox.crs) for geom in geoms],\n",
    "                                           out_shape=geobox.shape,\n",
    "                                           transform=geobox.affine,\n",
    "                                           all_touched=all_touched,\n",
    "                                           invert=invert)\n",
    "    if chunks is not None:\n",
    "        data = dask.array.from_array(data, chunks=tuple(chunks[d] for d in geobox.dims))\n",
    "        \n",
    "    coords = [xr.DataArray(data=coord.values, name=dim, dims=[dim], attrs={'units': coord.units}) \n",
    "              for dim, coord in geobox.coords.items()]\n",
    "    return xr.DataArray(data, coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:18.754495Z",
     "start_time": "2019-09-15T22:51:18.744862Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_shapes(shape_file):\n",
    "    \"\"\"\n",
    "    Extract spatial inforamtion from polygons within shapefile\n",
    "    \"\"\"\n",
    "    with fiona.open(shape_file) as shapes:\n",
    "        crs = datacube.utils.geometry.CRS(shapes.crs_wkt)\n",
    "        for shape in shapes:\n",
    "            if shape['geometry'] is None:\n",
    "                continue\n",
    "            geom = datacube.utils.geometry.Geometry(shape['geometry'], crs=crs)\n",
    "            geom = geom.to_crs(CRS('EPSG:3577'))\n",
    "            yield geom, shape['properties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:18.783841Z",
     "start_time": "2019-09-15T22:51:18.758827Z"
    }
   },
   "outputs": [],
   "source": [
    "def fc_and_water_summary(data, mask_int):\n",
    "    \"\"\"\n",
    "    Calculate the percentage and the area of each FC component, \n",
    "    water, and unclassified data within each SA2 boundary. \n",
    "    \"\"\"\n",
    "    # Where there is water, set the FC bands to 0%\n",
    "    valid_data = numpy.isfinite(data['BS'])\n",
    "    fc = data[['PV', 'NPV', 'BS']].where(data.water==0, other=0)\n",
    "    fc['water'] = data.water.where(valid_data) * numpy.float32(100)\n",
    "    fc = fc.apply(lambda data_array: data_array.clip(0, 100).where(valid_data))\n",
    "    has_data = valid_data.groupby(valid_data.time.astype('datetime64[M]'), squeeze=False).sum(dim='time', skipna=False)\n",
    "    has_data = has_data.sum(dim=['x','y'], skipna=True) / (mask_int / 100)\n",
    "    \n",
    "    # Flatten to a monthly mean\n",
    "    fc = fc.groupby(fc.time.astype('datetime64[M]'), squeeze=False).mean(dim='time', skipna=True)\n",
    "    \n",
    "    # Calculate percentage of cover based on area of mask\n",
    "    percentage = fc.sum(dim=['x','y']) * (100 / mask_int)\n",
    "    for da in percentage.data_vars.values():\n",
    "        da.attrs['units'] = '%'\n",
    "    fc_sum = fc.sum(dim=['x','y'])\n",
    "    fc_sum = fc_sum.PV + fc_sum.NPV + fc_sum.BS + fc_sum.water\n",
    "    \n",
    "    #Calculate the amount of SA2 that is unclassified (e.g. not FC or water)\n",
    "    unclass_size = mask_int - fc_sum\n",
    "    fc['unclassified'] = unclass_size     \n",
    "    unclass_percentage = unclass_size * (100 / mask_int)\n",
    "    percentage['unclassified'] = unclass_percentage\n",
    "    \n",
    "    #Calculate area of fc components   \n",
    "    pixel_area_in_metres2 = 25 * 25\n",
    "    m2_to_km2 = (1 / 1_000_000)\n",
    "    percent_to_fraction = (1 / 100)\n",
    "    area = (fc * (pixel_area_in_metres2 * m2_to_km2 * percent_to_fraction)).sum(dim=['x','y'])\n",
    "    area = area.rename({'BS': 'BS_area', \n",
    "                        'PV': 'PV_area', \n",
    "                        'NPV': 'NPV_area', \n",
    "                        'water': 'water_area',\n",
    "                        'unclassified':'unclassified_area'})\n",
    "    for da in area.data_vars.values():\n",
    "        da.attrs['units'] = 'km^2'\n",
    "    fc = percentage.merge(area)   \n",
    "    fc['average_data_count'] = has_data\n",
    "    \n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:18.801855Z",
     "start_time": "2019-09-15T22:51:18.787295Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_stacked(ds, sa2_id,plot_title='title', show=True):\n",
    "    \"\"\"\n",
    "    Create and save a stacked plot to visualise FC components\n",
    "    \"\"\"\n",
    "    if not show:\n",
    "        plt.ioff()\n",
    "        \n",
    "    fig,ax = plt.subplots(figsize=(10,5))\n",
    "    ax.stackplot(ds.dropna(dim='time').time.data, \n",
    "                 ds.dropna(dim='time').PV,\n",
    "                 ds.dropna(dim='time').NPV, \n",
    "                 ds.dropna(dim='time').BS, \n",
    "                 ds.dropna(dim='time').water, \n",
    "                 ds.dropna(dim='time').unclassified, \n",
    "                 colors = ['darkolivegreen','olive','tan','darkblue','red'], \n",
    "                 labels=['Green veg','Brown veg','Bare soil','Water','Unclassified',])\n",
    "    plt.legend(loc='lower center', ncol = 5)\n",
    "    plt.title(f'FC components: SA2 ID {sa2_id}', size=12)\n",
    "    plt.ylabel('Percentage (%)', size=12) #Set Y label\n",
    "    plt.xlabel('Date', size=12) #Set X label\n",
    "    \n",
    "    plt.savefig(f'{output_path}/{sa2_id}_{plot_title}.png');\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Turn interactive back on\n",
    "    if not show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:18.813133Z",
     "start_time": "2019-09-15T22:51:18.805222Z"
    }
   },
   "outputs": [],
   "source": [
    "def month_splitter(start_year, end_year_inclusive):\n",
    "    \"\"\" \n",
    "    Split specified years into months \n",
    "    \"\"\"\n",
    "    yield from (str(p) for p in pd.period_range(start=pd.Period(start_year).start_time, \n",
    "                               end=pd.Period(end_year_inclusive).end_time, \n",
    "                               freq='M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:18.823532Z",
     "start_time": "2019-09-15T22:51:18.816545Z"
    }
   },
   "outputs": [],
   "source": [
    "def output_csv(input_ds, sa2_id, sa2_name, sa2_size, monthly_or_annual='frequency'):\n",
    "    \"\"\"\n",
    "    Save tabulated data into a csv\n",
    "    \"\"\"\n",
    "    input_ds = input_ds.to_dataframe()\n",
    "    input_ds.insert(0,'SA2_id', sa2_id)\n",
    "    input_ds.insert(1,'SA2_name', sa2_name)\n",
    "    input_ds.insert(2,'SA2_size', sa2_size)\n",
    "    input_ds.to_csv(f\"{output_path}/{name_of_output_file}_{monthly_or_annual}.csv\",mode='a',header=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:18.851912Z",
     "start_time": "2019-09-15T22:51:18.826386Z"
    }
   },
   "outputs": [],
   "source": [
    "#create FC tabulation function\n",
    "monthly_values = None\n",
    "\n",
    "def process_area(geometry, sa2_id, sa2_name, sa2_size, time_range):\n",
    "    \"\"\"Process the values\"\"\"\n",
    "    monthly_values = []\n",
    "    annual_values = []\n",
    "    mask = None\n",
    "    \n",
    "    # Virtual Products have a bug handling geometry objects, use this instead of `geopolygon=geometry` for now\n",
    "    search_terms = {\n",
    "        'x': (geometry.envelope.left, geometry.envelope.right),\n",
    "        'y': (geometry.envelope.top, geometry.envelope.bottom),\n",
    "        'crs': str(geometry.crs),\n",
    "    }  \n",
    "\n",
    "    for sub_time_range in month_splitter(time_range[0], time_range[-1]):\n",
    "        print(sub_time_range)\n",
    "        try:\n",
    "            data = fc_and_water.load(dc, dask_chunks=chunk_size, \n",
    "                                     time=sub_time_range, \n",
    "                                     **search_terms)\n",
    "        except ValueError:\n",
    "            print(f'    No data for {sub_time_range} , skipping...')\n",
    "            continue\n",
    "\n",
    "        if mask is None:\n",
    "            mask = geometry_mask([geometry], data.geobox, invert=True, chunks=data.chunks)\n",
    "            mask_int = mask * 1\n",
    "            mask_int = mask_int.sum() * 100\n",
    "            mask_int.load()\n",
    "        # Mask data\n",
    "        data = data.where(mask)\n",
    "        monthly_data = fc_and_water_summary(data, mask_int)\n",
    "        monthly_data = client.compute(monthly_data, sync=by_slice)\n",
    "        monthly_values.append(monthly_data)\n",
    "        \n",
    "    monthly_values = xr.concat(monthly_values, dim='time')\n",
    "    monthly_qa = xr.Dataset(coords={'time': monthly_values.time})\n",
    "    monthly_qa['has_values'] = (('time',), [1] * monthly_values.time.size)\n",
    "    monthly_qa['has_10'] = monthly_values['unclassified'] <= 10\n",
    "    monthly_values = monthly_values.where(monthly_values['unclassified'] < 10).dropna(dim='time')\n",
    "    annual_values = monthly_values.resample(time='1YS').mean(dim='time', skipna=True)\n",
    "    \n",
    "    # Replace the mean of the monthly obs count with the annual total obs count\n",
    "    annual_avg_data_count = monthly_values['average_data_count'].resample(time='1YS').sum(dim='time', skipna=True)\n",
    "    annual_values['average_data_count'] = annual_avg_data_count\n",
    "    annual_qa = monthly_qa.resample(time='1YS').sum(dim='time', skipna=True)\n",
    "    annual_qa = 12 - annual_qa\n",
    "    annual_values = annual_values.merge(annual_qa)\n",
    "    annual_values['time'] = annual_values.time.dt.year\n",
    "\n",
    "    print(f\"Calculation complete for annual values\")\n",
    "    \n",
    "    if create_stackplots == \"True\":\n",
    "        plot_stacked(monthly_values, sa2_id, plot_title='monthly_plot_wofs',show=False)\n",
    "        plot_stacked(annual_values, sa2_id, plot_title='annual_plot_wofs', show=False)\n",
    "    else:\n",
    "        print(f\"Do not make stackplot\")\n",
    "    \n",
    "    #append  outputs to csv\n",
    "    print(\"All data loaded, save to csv\")\n",
    "    output_csv(monthly_values, sa2_id, sa2_name, sa2_size, monthly_or_annual='monthly')\n",
    "    output_csv(annual_values, sa2_id, sa2_name, sa2_size, monthly_or_annual='annual')\n",
    "    \n",
    "    return annual_values, monthly_values\n",
    "    \n",
    "    print(f\"SA2 {sa2_id} done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Set up the query\n",
    "For each year and polygon query the product, apply the geometry mask and compute the fractional cover stats\n",
    "\n",
    "Using `client.compute()` lets us use the monthly results in calculating the annual results at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:30.631004Z",
     "start_time": "2019-09-15T22:51:18.855234Z"
    }
   },
   "outputs": [],
   "source": [
    "#The following code does not need to be edited.\n",
    "\n",
    "#Obtain spatial information from shapefile\n",
    "shape_file = os.path.expanduser(f'{shapefile_path}')\n",
    "shapes = list(get_shapes(shape_file))\n",
    "\n",
    "#A list of sa2 that do not have a spatial footprint (e.g. \"Migratory - Offshore - Shipping\" OR \"No usual address\") \n",
    "#or SA2 that are outside of the DEA coverage, e.g. Christmas Island or Norfolk Island. \n",
    "sa2_outside_DEA = ['901011001', '108031161', '901021002', '901041004', \n",
    "                   '197979799', '199999499', '297979799', '299999499', \n",
    "                   '397979799', '399999499', '497979799', '499999499', \n",
    "                   '597979799', '599999499', '697979799', '699999499', \n",
    "                   '797979799', '799999499', '897979799', '899999499', \n",
    "                   '997979799', '999999499']\n",
    "\n",
    "#remove the SA2 that are outside of the coverage of DEA FC dataset\n",
    "for sa2_outside in sa2_outside_DEA:\n",
    "    shapes = [(g,p) for g, p in shapes if str(p['SA2_MAIN16']) != sa2_outside]\n",
    "    \n",
    "print(len(shapes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:30.667071Z",
     "start_time": "2019-09-15T22:51:30.634516Z"
    }
   },
   "outputs": [],
   "source": [
    "#The follow code allows the use to choose one specific SA2 base. Hash/unhash the specific line of code.\n",
    "#The user can filter by things such as SA2 ID, or SA2 size.\n",
    "\n",
    "# shapes = [(g,p) for g, p in shapes if str(p['SA2_MAIN16']) == '801041120'] #choose based on SA2 ID\n",
    "# shapes = [(g,p) for g, p in shapes if int(p['AREASQKM16']) <= 0.99]  #choose based on SA2 size, smaller than value\n",
    "shapes = [(g,p) for g, p in shapes if 0 < int(p['AREASQKM16']) <= 20] #choose based on SA2 size, between two value\n",
    "\n",
    "print(len(shapes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create empty CSVs to save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:51:30.764504Z",
     "start_time": "2019-09-15T22:51:30.670196Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create empty CSV with specified headings\n",
    "header = ['DATE','SA2_MAIN16', 'SA2_NAME16', 'AREASQKM16','PV_PERCENTAGE','NPV_PERCENTAGE','BS_PERCENTAGE'\n",
    "          ,'WOFL_PERCENTAGE', 'UNCLASSIFIED_PERCENTAGE','PV_AREA_SQKM_ALBERS3577','NPV_AREA_SQKM_ALBERS3577',\n",
    "          'BS_AREA_SQKM_ALBERS3577','WOFL_AREA_SQKM_ALBERS3577','UNCLASSIFIED_AREA_SQKM_ALBERS3577', \n",
    "          'AVERAGE_OBSERVATION_COUNT']\n",
    "\n",
    "additional_annual_header = ['MONTHS_WITH_NO_DATA', 'MONTHS_WITH_LOW_DATA']\n",
    "\n",
    "error_header = ['','RUN_NAME','SA2_MAIN16','DATE_RANGE','ERROR_MESSAGE']\n",
    "\n",
    "with open(f\"{output_path}/{name_of_output_file}_annual.csv\",\"w\") as outcsv: #create csv to save output and add header text\n",
    "    writer = csv.writer(outcsv)\n",
    "    writer.writerow(header + additional_annual_header)\n",
    "    \n",
    "with open(f\"{output_path}/{name_of_output_file}_monthly.csv\",\"w\") as outcsv: #create csv to save output and add header text\n",
    "    writer = csv.writer(outcsv)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "with open(f\"{output_path}/{name_of_output_file}_error_log.csv\",\"w\") as outcsv: #create csv to save output\n",
    "    writer = csv.writer(outcsv)\n",
    "    writer.writerow(error_header)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run tabulation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:54:05.979779Z",
     "start_time": "2019-09-15T22:51:30.768424Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Run the tabulation process while saving metadata for the run.\n",
    "#The script loops through each SA2 and tabulates the FC results, saving to csv.\n",
    "#additionally the script has been set up to save run metadata and error information (if any)\n",
    "\n",
    "#save batch/run metadata\n",
    "run_start = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "count_of_sa2_complete = 0\n",
    "count_of_sa2_error = 0\n",
    "count_of_sa2_all = 0\n",
    "sa2_min_size = 500_0000\n",
    "sa2_max_size = 0\n",
    "\n",
    "#save batch information out to csv\n",
    "batch_start_information = pd.DataFrame([[name_of_output_file],[run_notes],[initials],[run_start]]).T\n",
    "batch_start_information.to_csv(f\"{output_path}/tabulate_FC_run_log.csv\",mode='a',header=False)\n",
    "\n",
    "#loop through SA2 and use tabulation script\n",
    "for geometry, properties in shapes:\n",
    "    count_of_sa2_all += 1 #metadata\n",
    "    \n",
    "    #print SA2 information to screen\n",
    "    print(count_of_sa2_all)\n",
    "    sa2_id = str(properties['SA2_MAIN16'])\n",
    "    sa2_name = str(properties['SA2_NAME16'])\n",
    "    sa2_size = str(properties['AREASQKM16'])\n",
    "    print(f\"SA2 ID: {sa2_id}, size: {sa2_size}km^2, time: {time_range}\")\n",
    "    \n",
    "    #loop through SA2 size and save the smallest and largest SA2 values\n",
    "    if float(sa2_min_size) > float(sa2_size):\n",
    "        sa2_min_size = sa2_size #metadata\n",
    "    if float(sa2_max_size) < float(sa2_size):\n",
    "        sa2_max_size = sa2_size #metadata\n",
    "    \n",
    "    #loop through months/years and tabulate FC results\n",
    "    try:\n",
    "        annual_vals, monthly_vals = process_area(geometry, sa2_id, sa2_name, sa2_size, time_range)\n",
    "        count_of_sa2_complete += 1\n",
    "    #if there are errors, save information to csv to record the name of SA2 and type of error\n",
    "    except Exception as e:\n",
    "            print(f\"Could not process {sa2_id}: {e}\")\n",
    "            error_information = pd.DataFrame([[name_of_output_file],[sa2_id],[time_range], [e]]).T\n",
    "            error_information.to_csv(f\"{output_path}/{name_of_output_file}_error_log.csv\",mode='a',header=False)\n",
    "            count_of_sa2_error += 1\n",
    "            client.restart()\n",
    "\n",
    "#save batch/run information\n",
    "run_end = datetime.now().strftime('%Y-%m-%d %H:%M:%S') #metadata\n",
    "run_seconds = (datetime.strptime(run_end,'%Y-%m-%d %H:%M:%S') - datetime.strptime(run_start,'%Y-%m-%d %H:%M:%S')).total_seconds() #metadata\n",
    "\n",
    "#save run/batch metadata information to csv\n",
    "batch_information = pd.DataFrame([[name_of_output_file],[run_notes],[initials],\n",
    "                                  [run_start],[run_end],[run_seconds],[sa2_max_size],\n",
    "                                  [sa2_min_size],[time_range],[count_of_sa2_all],\n",
    "                                  [count_of_sa2_complete], [count_of_sa2_error],]).T\n",
    "batch_information.to_csv(f\"{output_path}/tabulate_FC_run_log.csv\",mode='a',header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T22:54:05.984027Z",
     "start_time": "2019-09-15T22:50:53.273Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Cell finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
